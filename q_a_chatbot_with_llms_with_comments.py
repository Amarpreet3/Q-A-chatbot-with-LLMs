# -*- coding: utf-8 -*-
"""Q/A chatbot with LLMs with commants

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/q-a-chatbot-with-llms-with-commants-9496ffad-1fba-482b-8aa0-1daea4c96b43.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240612/auto/storage/goog4_request%26X-Goog-Date%3D20240612T154724Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da5026a4ed18026ee500e857662c05ca4405ab2c7858afb1f75a482e99574c327548661312d9d664b83aff333faf2ebbe7f32ba389d44048a152f030201526ec8bc3d81a5bd50176a8a1258994b24d04f19c2dce5b9c6b4a5781487fb473467975269976cc41602b400d3a621c67d747d0ebd6df387e3e082775bf4753377b59b521066c42a7c60e5a386659708f914767407013c00ca2fb9db5d662f0e45ec75911e6dc3846570ff26e73ad5186fa9f72d9fbed498c48f9ba83156f3bf40326c061f4d0e49b9308cb67556814ffc431727943622e1c4158b8947df8a77ff4204dbebefa06217971248fc482bd1683aec04ec7fe477920f5a0f0887bb74b11af7

# Overview

- Use [Langchain](https://python.langchain.com/en/latest/index.html) to <font color='orange'> build a chatbot that can answer questions about books or any pdf files.
- **<font color='orange'>Flexible and customizable RAG pipeline (Retrieval Augmented Generation)</font>**
- Experiment with various LLMs (Large Language Models)
- Use [FAISS vector store](https://python.langchain.com/docs/integrations/vectorstores/faiss) to store text embeddings created with [Sentence Transformers](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) from ðŸ¤—. FAISS runs on GPU and it is much faster than Chroma
- Use [Retrieval chain](https://python.langchain.com/docs/modules/data_connection/retrievers/) to retrieve relevant passages from embedded text
- Summarize retrieved passages
- Leverage Kaggle dual GPU (2 * T4) with [Hugging Face Accelerate](https://huggingface.co/docs/accelerate/index)
- Chat UI with [Gradio](https://www.gradio.app/guides/quickstart)

**<font color='green'>No need to create any API key to use this notebook! Everything is open source.</font>**


### Models

- [TheBloke/wizardLM-7B-HF](https://huggingface.co/TheBloke/wizardLM-7B-HF)
- [daryl149/llama-2-7b-chat-hf](https://huggingface.co/daryl149/llama-2-7b-chat-hf)
- [daryl149/llama-2-13b-chat-hf](https://huggingface.co/daryl149/llama-2-13b-chat-hf)
- [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)

![download.png](attachment:548aeeee-07e3-44e5-ba4c-cf13afeab9bc.png)

img source: HinePo
"""

! nvidia-smi -L

"""# Installs"""

#The NVIDIA System Management Interface (nvidia-smi) is a command line utility, based on top of the NVIDIA Management Library (NVML), intended to aid in the management and monitoring of NVIDIA GPU devices.
#Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and training state-of-the-art text and image embedding models.
#LangChain is a framework for developing applications powered by large language models (LLMs).
#tiktoken is a fast open-source tokenizer by OpenAI.
#pypdf. pypdf is a free and open-source pure-python PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files.
#Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size.
#Instructor-embedding: an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning.
#A Transformer is a type of deep learning architecture that uses an attention mechanism to process text sequences.
#Accelerate was created for PyTorch users who like to write the training loop of PyTorch models but are reluctant to write and maintain the boilerplate code needed to use multi-GPUs/TPU/fp16.
#Bitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers and quantization functions.
#LangChain Community contains third-party integrations that implement the base interfaces defined in LangChain Core, making them ready-to-use in any LangChain application.

# Install the desired version of sentence_transformers within the virtual environment
!pip install sentence_transformers==2.2.2

# Clear the output to keep the notebook clean
from IPython.display import clear_output
clear_output()

! pip install -qq -U langchain

! pip install -qq -U tiktoken

! pip install -qq -U pypdf

! pip install -qq -U faiss-gpu

! pip install -qq -U InstructorEmbedding

! pip install -qq -U transformers

! pip install -qq -U accelerate

! pip install -qq -U bitsandbytes

!pip install -U langchain-community

"""# Imports"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# import warnings
# warnings.filterwarnings("ignore")

# Importing the os module for interacting with the operating system
import os

# Importing the glob module to find all the pathnames matching a specified pattern
import glob

# Importing the textwrap module for formatting text
import textwrap

# Importing the time module to handle time-related tasks
import time

# Importing the langchain module for building language model chains
import langchain

# Importing PyPDFLoader for loading PDF documents
from langchain.document_loaders import PyPDFLoader

# Importing DirectoryLoader for loading all documents from a directory
from langchain.document_loaders import DirectoryLoader

### Importing modules for text splitting
from langchain.text_splitter import RecursiveCharacterTextSplitter

### Importing modules for prompts and LLM chains
from langchain import PromptTemplate, LLMChain

### Importing modules for vector stores
from langchain.vectorstores import FAISS

### Importing modules for models
from langchain.llms import HuggingFacePipeline
from langchain.embeddings import HuggingFaceInstructEmbeddings

### Importing modules for retrievers
from langchain.chains import RetrievalQA

# Importing the torch module for working with PyTorch, an open source machine learning library
import torch

# Importing the transformers module for working with Transformer models
import transformers

# Importing specific classes and functions from the transformers module
from transformers import (
    AutoTokenizer,        # For loading pre-trained tokenizers
    AutoModelForCausalLM, # For loading pre-trained causal language models
    BitsAndBytesConfig,   # For configuring quantization settings for models
    pipeline              # For creating inference pipelines
)

# Clearing the output (commonly used in Jupyter notebooks)
from IPython.display import clear_output
clear_output()

# Printing the versions of the imported libraries
print('langchain:', langchain.__version__)
print('torch:', torch.__version__)
print('transformers:', transformers.__version__)

# Finding and sorting all the file paths in the specified directory
sorted(glob.glob('/kaggle/input/harrypotter2/*'))

"""# CFG

- CFG class enables easy and organized experimentation
"""

class CFG:
    # Language Model Configuration
    model_name = 'llama2-13b-chat'  # Options: 'wizardlm', 'llama2-7b-chat', 'llama2-13b-chat', 'mistral-7B'
    temperature = 0  # Controls randomness of model's outputs
    top_p = 0.95  # Controls diversity of model's outputs
    repetition_penalty = 1.15  # Penalizes model for repeating itself in the output

    # Text Splitting Configuration
    split_chunk_size = 800  # Size of chunks to split text into for processing
    split_overlap = 0  # Overlap between chunks

    # Embeddings Configuration
    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'  # Repository for the embeddings model

    # Similar Passages Configuration
    k = 6  # Number of similar passages to retrieve

    # File Paths
    PDFs_path = '/kaggle/input/harrypotter2/'  # Path to the PDF files containing the text data
    Embeddings_path = '/kaggle/input/faiss-hp-sentence-transformers'  # Path to the embeddings data
    Output_folder = './harry-potter-vectordb'  # Folder to save the output data

"""# Define model"""

def get_model(model=CFG.model_name):
    """
    Downloads and initializes a specific model based on the `model` parameter.

    Args:
        model (str): The name of the model to use. Defaults to `CFG.model_name`.

    Returns:
        tuple: A tuple containing the initialized tokenizer, model, and `max_len` parameter.
    """
    # Print a message indicating which model is being downloaded
    print('\nDownloading model:', model, '\n\n')

    # Default values for tokenizer, model, and max_len
    tokenizer, model, max_len = None, None, None

    # Check if the model is 'wizardlm'
    if model == 'wizardlm':
        # Set the model repository for 'wizardlm'
        model_repo = 'TheBloke/wizardLM-7B-HF'

        # Initialize the tokenizer for 'wizardlm'
        tokenizer = AutoTokenizer.from_pretrained(model_repo)

        # Configure the quantization for 'wizardlm'
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        # Initialize the model for 'wizardlm'
        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            quantization_config=bnb_config,
            device_map='auto',
            low_cpu_mem_usage=True
        )

        # Set the maximum length for 'wizardlm'
        max_len = 1024

    # Check if the model is 'llama2-7b-chat' or 'llama2-13b-chat'
    elif model in ['llama2-7b-chat', 'llama2-13b-chat']:
        # Set the model repository for 'llama2-7b-chat' or 'llama2-13b-chat'
        model_repo = f'daryl149/{model}-hf'

        # Initialize the tokenizer for 'llama2-7b-chat' or 'llama2-13b-chat'
        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)

        # Configure the quantization for 'llama2-7b-chat' or 'llama2-13b-chat'
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        # Initialize the model for 'llama2-7b-chat' or 'llama2-13b-chat'
        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            quantization_config=bnb_config,
            device_map='auto',
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )

        # Set the maximum length for 'llama2-7b-chat' or 'llama2-13b-chat'
        max_len = 2048 if model == 'llama2-7b-chat' else 8192

    # Check if the model is 'mistral-7B'
    elif model == 'mistral-7B':
        # Set the model repository for 'mistral-7B'
        model_repo = 'mistralai/Mistral-7B-v0.1'

        # Initialize the tokenizer for 'mistral-7B'
        tokenizer = AutoTokenizer.from_pretrained(model_repo)

        # Configure the quantization for 'mistral-7B'
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        # Initialize the model for 'mistral-7B'
        model = AutoModelForCausalLM.from_pretrained(
            model_repo,
            quantization_config=bnb_config,
            device_map='auto',
            low_cpu_mem_usage=True
        )

        # Set the maximum length for 'mistral-7B'
        max_len = 1024

    # Handle the case when the model is not implemented
    else:
        print("Not implemented model (tokenizer and backbone)")

    # Return the initialized tokenizer, model, and max_len
    return tokenizer, model, max_len

# Commented out IPython magic to ensure Python compatibility.
# # Measure the execution time of the following code block
# %%time
# 
# # Get the tokenizer, model, and max_len using the get_model function with model set to CFG.model_name
# tokenizer, model, max_len = get_model(model=CFG.model_name)
# 
# # Clear the output of the cell
# clear_output()
#

#The model.eval() method is used to set the model to evaluation mode. In PyTorch, this is important when you have layers like Dropout or BatchNorm which behave differently during training and evaluation.
model.eval()

### check how Accelerate split the model across the available devices (GPUs)
model.hf_device_map

"""# ðŸ¤— pipeline

- Hugging Face pipeline
"""

### Create a Hugging Face pipeline for text generation
pipe = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    pad_token_id=tokenizer.eos_token_id,
#     do_sample=True,
    max_length=max_len,
    temperature=CFG.temperature,
    top_p=CFG.top_p,
    repetition_penalty=CFG.repetition_penalty
)

### Create a langchain pipeline using the Hugging Face pipeline
llm = HuggingFacePipeline(pipeline=pipe)

llm

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ### testing model, not using the books yet
# ### answer is not necessarily related to books
# query = "Give me 5 examples of cool potions and explain what they do"
# llm.invoke(query)

"""# ðŸ¦œðŸ”— Langchain

- Multiple document retriever with LangChain
"""

CFG.model_name

"""# Loader

- [Directory loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory) for multiple files
- This step is not necessary if you are just loading the vector database
- This step is necessary if you are creating embeddings. In this case you need to:
    - load de PDF files
    - split into chunks
    - create embeddings
    - save the embeddings in a vector store
    - After that you can just load the saved embeddings to do similarity search with the user query, and then use the LLM to answer the question
    
You can comment out this section if you use the embeddings I already created.
"""

# Commented out IPython magic to ensure Python compatibility.
# # Measure the execution time of the following code block
# %%time
# 
# # Load PDF documents using DirectoryLoader
# loader = DirectoryLoader(
#     CFG.PDFs_path,
#     glob="./*.pdf",
#     loader_cls=PyPDFLoader,
#     show_progress=True,
#     use_multithreading=True
# )
# 
# # Load the documents
# documents = loader.load()
#

print(f'We have {len(documents)} pages in total')

documents[8].page_content

"""# Splitter

- Splitting the text into chunks so its passages are easily searchable for similarity
- This step is also only necessary if you are creating the embeddings
- [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/reference/modules/document_loaders.html?highlight=RecursiveCharacterTextSplitter#langchain.document_loaders.MWDumpLoader)
"""

# Create a RecursiveCharacterTextSplitter for splitting text
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CFG.split_chunk_size,
    chunk_overlap=CFG.split_overlap
)

# Split the documents into chunks
texts = text_splitter.split_documents(documents)

# Print the number of chunks created
print(f'We have created {len(texts)} chunks from {len(documents)} pages')

"""# Create Embeddings


- Embedd and store the texts in a Vector database (FAISS)
- [LangChain Vector Stores docs](https://python.langchain.com/docs/modules/data_connection/vectorstores/)
- [FAISS - langchain](https://python.langchain.com/docs/integrations/vectorstores/faiss)
- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks - paper Aug/2019](https://arxiv.org/pdf/1908.10084.pdf)
- [This is a nice 4 minutes video about vector stores](https://www.youtube.com/watch?v=dN0lsF2cvm4)

___

- If you use Chroma vector store it will take ~35 min to create embeddings
- If you use FAISS vector store on GPU it will take just ~3 min

___

We need to create the embeddings only once, and then we can just load the vector store and query the database using similarity search.

Loading the embeddings takes only a few seconds.

I uploaded the embeddings to a Kaggle Dataset so we just load it from [here](https://www.kaggle.com/datasets/hinepo/faiss-hp-sentence-transformers).
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# ### we create the embeddings only if they do not exist yet
# if not os.path.exists(CFG.Embeddings_path + '/index.faiss'):
# 
#     ### download embeddings model
#     embeddings = HuggingFaceInstructEmbeddings(
#         model_name = CFG.embeddings_model_repo,
#         model_kwargs = {"device": "cuda"}
#     )
# 
#     ### create embeddings and DB
#     vectordb = FAISS.from_documents(
#         documents = texts,
#         embedding = embeddings
#     )
# 
#     ### persist vector database
#     vectordb.save_local(f"{CFG.Output_folder}/faiss_index_hp") # save in output folder
# #     vectordb.save_local(f"{CFG.Embeddings_path}/faiss_index_hp") # save in input folder

"""If creating embeddings, remember that on Kaggle we can not write data to the input folder.

So just write (save) the embeddings to the output folder and then load them from there.

# Load vector database

- After saving the vector database, we just load it from the Kaggle Dataset I mentioned
- Obviously, the embeddings function to load the embeddings must be the same as the one used to create the embeddings
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from langchain.embeddings import HuggingFaceInstructEmbeddings
# from langchain.vectorstores import FAISS
# 
# # Download embeddings model
# embeddings = HuggingFaceInstructEmbeddings(
#     model_name=CFG.embeddings_model_repo,
#     model_kwargs={"device": "cuda"}
# )
# 
# # Load vector DB embeddings
# vectordb = FAISS.load_local(
#     CFG.Embeddings_path,  # from input folder
#     embeddings,
#     allow_dangerous_deserialization=True  # Allow deserialization
# )
# 
# from IPython.display import clear_output
# clear_output()
# 
# print("FAISS vector database loaded successfully!")
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# ### download embeddings model
# embeddings = HuggingFaceInstructEmbeddings(
#     model_name = CFG.embeddings_model_repo,
#     model_kwargs = {"device": "cuda"}
# )
# 
# ### load vector DB embeddings
# vectordb = FAISS.load_local(
#     CFG.Embeddings_path, # from input folder
# #     CFG.Output_folder + '/faiss_index_hp', # from output folder
#     embeddings
# )
# 
# clear_output()

### test if vector DB was loaded correctly
vectordb.similarity_search('magic creatures')

"""# Prompt Template

- Custom prompt
"""

prompt_template = """
Don't try to make up an answer, if you don't know just say that you don't know.
Answer in the same language the question was asked.
Use only the following pieces of context to answer the question at the end.

{context}

Question: {question}
Answer:"""


PROMPT = PromptTemplate(
    template = prompt_template,
    input_variables = ["context", "question"]
)

# llm_chain = LLMChain(prompt=PROMPT, llm=llm)
# llm_chain

"""# Retriever chain

- Retriever to retrieve relevant passages
- Chain to answer questions
- [RetrievalQA: Chain for question-answering](https://python.langchain.com/docs/modules/data_connection/retrievers/)
"""

retriever = vectordb.as_retriever(search_kwargs = {"k": CFG.k, "search_type" : "similarity"})

qa_chain = RetrievalQA.from_chain_type(
    llm = llm,
    chain_type = "stuff", # map_reduce, map_rerank, stuff, refine
    retriever = retriever,
    chain_type_kwargs = {"prompt": PROMPT},
    return_source_documents = True,
    verbose = False
)

### testing MMR search
question = "Which are Hagrid's favorite animals?"
vectordb.max_marginal_relevance_search(question, k = CFG.k)

### testing similarity search
question = "Which are Hagrid's favorite animals?"
vectordb.similarity_search(question, k = CFG.k)

"""# Post-process outputs

- Format llm response
- Cite sources (PDFs)
- Change `width` parameter to format the output
"""

def wrap_text_preserve_newlines(text, width=700):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text


def process_llm_response(llm_response):
    ans = wrap_text_preserve_newlines(llm_response['result'])

    sources_used = ' \n'.join(
        [
            source.metadata['source'].split('/')[-1][:-4]
            + ' - page: '
            + str(source.metadata['page'])
            for source in llm_response['source_documents']
        ]
    )

    ans = ans + '\n\nSources: \n' + sources_used
    return ans

def llm_ans(query):
    start = time.time()

    llm_response = qa_chain.invoke(query)
    ans = process_llm_response(llm_response)

    end = time.time()

    time_elapsed = int(round(end - start, 0))
    time_elapsed_str = f'\n\nTime elapsed: {time_elapsed} s'
    return ans + time_elapsed_str

"""# Ask questions

- Question Answering from multiple documents
- Invoke QA Chain
- Talk to your data
"""

CFG.model_name

query = "Which challenges does Harry face during the Triwizard Tournament?"
print(llm_ans(query))

query = "Why do the Malfoys look so unhappy with their lot? "
print(llm_ans(query))

query = "What are horcrux?"
print(llm_ans(query))

query = "Give me 5 examples of cool potions and explain what they do"
print(llm_ans(query))

"""# Gradio Chat UI

- **<font color='orange'>At the moment this part only works on Google Colab. Gradio and Kaggle started having compatibility issues recently.</font>**
- If you plan to use the interface, it is preferable to do so in Google Colab
- I'll leave this section commented out for now
- Chat UI prints below

___

- Create a chat UI with [Gradio](https://www.gradio.app/guides/quickstart)
- [ChatInterface docs](https://www.gradio.app/docs/chatinterface)
- The notebook should be running if you want to use the chat interface
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

! pip install --upgrade gradio -qq
clear_output()

# Update typing_extensions to the latest version
#!pip install --upgrade typing_extensions

# Reinstall gradio to ensure all dependencies are met
#!pip install --upgrade gradio

# Import gradio and print its version
#import gradio as gr
#print(gr.__version__)

#import gradio as gr
#print(gr.__version__)

# def predict(message, history):
#     # output = message # debug mode

#     output = str(llm_ans(message)).replace("\n", "<br/>")
#     return output

# demo = gr.ChatInterface(
#     predict,
#     title = f' Open-Source LLM ({CFG.model_name}) for Harry Potter Question Answering'
# )

# demo.queue()
# demo.launch()

"""![image.png](attachment:413fe7a3-6534-45b5-b6e3-7fc86e982cf1.png)

![image.png](attachment:976f4bf4-7626-4d4a-b773-3eebd7e9f000.png)

# Conclusions

- Feel free to fork and optimize the code. Lots of things can be improved.

- Things I found had the most impact on models output quality in my experiments:
    - Prompt engineering
    - Bigger models
    - Other models families
    - Splitting: chunk size, overlap
    - Search: Similarity, MMR, k
    - Pipeline parameters (temperature, top_p, penalty)
    - Embeddings function
    - LLM parameters (max len)


- LangChain, Hugging Face and Gradio are awesome libs!

- **<font color='orange'>If you liked this notebook, don't forget to show your support with an Upvote!</font>**

- In case you are interested in LLMs, I also have some other notebooks you might want to check:

    - [Instruction Finetuning](https://www.kaggle.com/code/hinepo/llm-instruction-finetuning-wandb)
    - [Preference Finetuning - LLM Alignment](https://www.kaggle.com/code/hinepo/llm-alignment-preference-finetuning)
    - [Synthetic Data for Finetuning](https://www.kaggle.com/code/hinepo/synthetic-data-creation-for-llms)
    - [Safeguards and Guardrails](https://www.kaggle.com/code/hinepo/llm-safeguards-and-guardrails)
    
___

ðŸ¦œðŸ”—ðŸ¤—

![image.png](attachment:68773819-4358-4ded-be3e-f1d275103171.png)
"""